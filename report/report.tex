\documentclass{article}
\usepackage[left=2.7cm, right=2.7cm, top=3cm]{geometry} % Change margins
\usepackage{mathtools} % falign
\usepackage{amsmath} % Basic maths
\usepackage{amssymb} % For some symbols
\usepackage{bm} % bold math
\usepackage{tikz} % For drawing trees
\usepackage{tikz-qtree} % For drawing trees
\usetikzlibrary{trees, positioning, arrows.meta, calc} % For drawing trees
\usepackage{xcolor} % Colouring equations
\usepackage{fancyhdr} % Cool headers
\usepackage{tabularx} % Fixed tables
\usepackage{graphicx}
\usepackage{multirow} % table merge row
\usepackage{color, colortbl} % colouring
\usepackage{tcolorbox} % for boxed stuffs, text + math mode
\usepackage{alltt} % better verbatim
\usepackage{wrapfig} % wrapping images with env wrapfigure
\usepackage[hidelinks]{hyperref}
\usepackage{booktabs} % better tables
\usepackage{float} % float tables
\usepackage[underline=false]{pgf-umlsd} % uml sequence diagrams
\restylefloat{table}
\usepackage{minted} % code highlighting
\usepackage{longtable} % tables that span multiple pages
\usepackage{enumitem} % \begin{enumerate}[label=(\alph*)]
\usepackage{pgfplots}
\usepackage{xifthen}% provides \isempty test
\usepackage[font=small,labelfont=bf]{caption}

\usepackage{subfig}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\pgfplotsset{compat=1.16} % Backwards compatibility

\usepackage{xargs} % Use more than one optional parameter in a new commands

\usepackage[sorting=none]{biblatex}
\addbibresource{refs.bib}

% A really nice TODO package and helpers
% https://tex.stackexchange.com/questions/9796/how-to-add-todo-notes
\setlength {\marginparwidth }{2cm}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\todoany}[2][1=]{\todo[inline,#1]{[\sectionlabel{}]~#2}}
\newcommandx{\todojim}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,inline,#1]{[\sectionlabel{}]~\textbf{Jim}:~#2}}
\newcommandx{\tododuke}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,inline,#1]{[\sectionlabel{}]~\textbf{Duke}:~#2}}
\newcommandx{\tododhruv}[2][1=]{\todo[linecolor=green,backgroundcolor=green!25,bordercolor=green,inline,#1]{[\sectionlabel{}]~\textbf{Dhruv}:~#2}}

\hypersetup{
  colorlinks=true,
  urlcolor=blue,
  linkcolor=black,
  citecolor=black
}

% circled numbers
\newcommand*\circled[1]{\tikz[baseline = (char.base)]{%
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}%

% For use in align*
\newcommand*\mathcomment[1]{&&{#1}}
\newcommand*\mathcommentt[1]{&&{\text{#1}}}

\usepackage{amsthm} % For Proofs
\newtheorem*{remark}{Remark}
\renewcommand\qedsymbol{}

\begin{document}

\pagestyle{fancy}
\fancyhf{}
\lhead{COMP9491: Project Report}
\rhead{Duck News Reporters}
\cfoot[C]{\thepage}

% L{3cm} for a ragged right paragraph in tabular tables.
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{0.5pt}

\newcommand{\sectiontitle}{}
\newcommand{\subsectiontitle}{}
\newcommand{\sectionlabel}{\ifdefempty{\subsectiontitle}{\sectiontitle}{\sectiontitle/\subsectiontitle}}
\newcommand{\newsection}[1]{\section{#1}\renewcommand{\sectiontitle}{#1}\renewcommand{\subsectiontitle}{}}
\newcommand{\newsubsection}[1]{\subsection{#1}\renewcommand{\subsectiontitle}{#1}}

% \begin{noindent}
\newcommand{\articlecontent}[1]{%
	\ifthenelse{\equal{#1}{118real}}{FBI Director James Comey said Sunday that
	the bureau won't change the conclusion it made in July after it examined
	newly revealed emails related to the Hillary Clinton probe.

	``Based on our review, we have not changed our conclusions that we
	expressed in July with respect to Secretary Clinton'' Comey wrote in a
	letter to 16 members of Congress. [...]
	}{}%
  \ifthenelse{\equal{#1}{1fake}}{Hillary in hot water over her email server,
  again. Sacramento, CA — Democratic nominee Hillary Clinton is in hot water
  again after nearly 5 million uncounted California electronic ballots were
  found on her email server by the F.B.I. The majority of those ballots cast
  were by Bernie Sanders supporters. [...] }{}%
  \ifthenelse{\equal{#1}{128real}}{[...]I have a prediction. I know exactly what
  November 9 will bring. Another day of God’s perfect sovereignty.

  He will still be in charge. His throne will still be occupied. He will still
  manage the affairs of the world. Never before has His providence depended on a
  king, president, or ruler. And it won’t on November 9, 2016. “The LORD can
  control a king’s mind as he controls a river; he can direct it as he pleases”
  (Proverbs 21:1 NCV).
  
  On one occasion the Lord turned the heart of the King of Assyria so that he
	aided them in the construction of the Temple.  On another occasion, he stirred
	the heart of Cyrus to release the Jews to return to Jerusalem. [...]}{}%
  \ifthenelse{\equal{#1}{2fake}}{Washington, D.C. – South African Billionaire,
  Femi Adenugame, has released a statement offering to help African-Americans
  leave the United States if Donald Trump is elected president. According to
  reports, he is offering \$1 Million, a home and car to every Black family who
  wants to come to South Africa.

  Concerns about Donald Trump becoming president has prompted a South African
  billionaire to invest his fortune in helping African-Americans leave the
  United States to avoid further discrimination and inequality. [...]}{}%
  \ifthenelse{\equal{#1}{10fake}}{The Internet is buzzing today after white
  supremacist presidential candidate Donald Trump was caught by hotel staff
  snorting cocaine.

  Maria Gonzalez an employee at the Folks INN \& Suites Hotel in Phoenix brought
  room service to his room witnessed it all.
  
  ``When I walked in I saw 3 naked prostitutes and maybe 100,000 in hundred
  dollars bills and a mountain of white powder on the table, I thought it was a
  dog on the floor sleep but it was his hair piece, he was bald and sweating
  like crazy.'' [...]}{}%
	\ifthenelse{\equal{#1}{15fake}}{After hearing about 200 Marines left
	stranded after returning home from Operation Desert Storm back in 1991,
	Donald J.Trump came to the aid of those Marines by sending one of his planes
	to Camp Lejuene, North Carolina to transport them back home to their
	families in Miami, Florida.

	Corporal Ryan Stickney was amongst the group that was stuck in North
	Carolina and could not make their way back to their homes. [...]
	}{}%
  \ifthenelse{\equal{#1}{34fake}}{It has been more than fifteen years since Rage
  Against The Machine have released new music. The members of the band have
  involved themselves in various other projects during their lengthy hiatus, but
  one pressing issue has forced the band to team up once again.

  In a statement posted online, Rage Against The Machine announced they would be
	releasing a brand new album aimed at spreading awareness about ``how awful
	Donald Trump is''. [...]
	}{}%
}
\newcommand{\articletitle}[1]{%
	\ifthenelse{\equal{#1}{118real}}{FBI Completes Review of Newly Revealed Hillary Clinton Emails Finds No Evidence of Criminality}{}%
  \ifthenelse{\equal{#1}{1fake}}{5 Million Uncounted Sanders Ballots Found On Clinton's Email Server}{}%
}
% \end{noindent}

\title{\textbf{Duck News Reporters: Automated fake news detection through contextual similarity comparison}\\\vspace*{12pt}\large{COMP9491: Applied Artificial Intelligence --- Project Report}}
\author{%
  Dhruv Agrawal\\
  \texttt{z5361800@unsw.edu.au}
  \and
  Duke Nguyen\\
  \texttt{z5398432@unsw.edu.au}
  \and
  Jim Tang\\
  \texttt{z5208565@unsw.edu.au}
}

\maketitle
\thispagestyle{empty}

\listoftodos % DELETE THIS BEFORE SUBMIT

\newsection{Introduction}

\todoany{Describe the problem domain and aim of study, briefly introduce the developed methods and summarise your experimental findings}

As the distribution of news shifts towards social media, there is a rise in the dissemination of fake news. We define fake news as the creation of information presented as legitimate journalism that describes a fictitious event or fabricates details of an event in an attempt to mislead readers. This phenomenon became a major focal point in journalism during the 2016 US elections with political parties labelling many publications and articles as fake news. There are two huge issues with how this occurred in 2016

\begin{enumerate}
    \item Many prominent political figures highlighted any news they disagreed with as fake news. This led to the political isolation of the party, whereby any news that looked at them unfavourably had the potential to be dismissed as fake news This reduced the accountability of political figures in the US, a country where federal legislation has a sweeping impact across the country and the rest of the world.
    \item There was a lack of fake news detection tools on social media and due to the polarisation of the media climate, it was extremely difficult for social media to regulate articles published on the platforms or remove factually incorrect articles posted or shared by politicians.
\end{enumerate}

Since then, there have been many attempts to introduce ways to deal with these issues such as Politifact which manually reviews articles and social media posts for factual correctness. It posts its findings on its website and is easily accessible for people. Other similar websites exist but the reason manual fact-checking tools are not as prominent in spaces with high amounts of fake news is because it is impossible for manual review tools to scale to the number of news articles and journalistic social media posts published every day.

There are also many automated fake news detection algorithms which rely on linguistic features of the text, comparisons between the title of the article and its content, the medium of transmission, and any suspicious activity linked with its engagement online. These tools have become more effective since COVID and Twitter employs its own automated algorithms to automatically label articles about certain topics it chooses as fake news. However, these tools are known to be very unreliable as it is increasingly common for fake news to read the same way as real news articles and be shared by humans.

Therefore, in order for fake news detection to become more widespread and effective in combating fake news, there are a few different criteria it must fulfil

\begin{enumerate}
    \item The algorithms need to automatically classify news as real or fake so that they can scale with the growth of social media and the increase in fake news dissemination.
    \item The algorithms need to incorporate current methods of fake news detection as these have been highly researched and are effective in many situations such as when fake news has been automatically generated or constructed in a highly polarised manner designed to provoke intense responses from readers.
    \item An additional feature that looks at the content and meaning of the article beyond how has been written must be used to combat fake news that is well-written and designed to look like real news.
    \item The dataset used to train and assess the algorithms must contain real and fake articles that are written in the same style so that it is not apparent simply from the way an article is written whether it is real or fake.
\end{enumerate}

Our approach improves upon existing approaches and aims to combine the above criteria to analyse both the content and style of articles and make a significantly more informed decision on its classification. The model is restricted to a binary classification - it outputs either real or fake rather than giving a confidence metric of an article's legitimacy. This is done as the aim is for the tool be easily adopted and focusing on the simplicity of the input and output is a priority.

We compiled a list of commonly used linguistic features for fake news detection. Multiple different pairings of features were formed and analysis was conducted to determine the most effective linguistic features for the task. This takes existing research into fake news detection and puts our model in line with current methods. A new feature - similarity - is used to achieve the third criterion above. At a high level, this feature compares the queried article to other articles on Google news which are at the top of Google's searches. As these have high PageRank scores, the model can be confident that they are real articles and it compares the similarity of the content between the queries article and each of these top searched articles. This is done as a way for the model to infer context before making a judgement on an article's legitimacy. This approach brings out model in line with the way humans manually fact-check articles which usually involves finding known trustworthy sources and comparing the content between the articles to determine whether the queried one is consistent with the trustworthy ones.

Through this research, we have analysed and determined the most effective linguistic features for fake news detection and shown that the use of similarity as a metric is effective in building upon these current metrics to increase accuracy. We have also compared the use of the similarity metric with different machine learning classifiers and discovered that it greatly increases the accuracy of less complex machine learning methods and brings their performance in line with complex models.

\newsection{Related work}

\tododhruv{Describe the current state-of-the-art or related literature in this problem domain}

\newsection{Methods}

\begin{minipage}{\textwidth}
  \begin{wrapfigure}{r}{0.65\textwidth}
    \vspace*{-20pt}
    \centering
    \includegraphics[width=0.6\textwidth]{img/pipeline.png}
    \caption{Our classification pipeline.}
    \label{pipeline}
  \end{wrapfigure}

  Figure~\ref{pipeline} shows our mostly linear classification pipeline. After preprocessing and tokenization, we extract contextual articles which are fed into a similarity model to form our first feature. Additionally, non-latent features from raw text and BERT embeddings form the rest of our features. The concatenation of all the features are fed into our classification models which infers a binary classification label.
\end{minipage}

\newsubsection{Preprocessing and tokenization}

Before extracting any features, we will preprocess our input and convert the long form text into tokens. We perform the following preprocessing methods in order:

\begin{quote}
  \textbf{Remove non-ascii:}\quad Our input articles contained unnecessary unicode tokens such as unicode double quotation marks. These can be removed safely since they do not add any extra semantics to the input articles and may confuse feature extraction.

  \textbf{Convert to lowercase:}\quad In our research, we converted all text to lowercase. However upon further analysis, converting all text to lowercase hid acronyms such as ``US'' which could have affected the main themes of the text. Further, all proper nouns such as names and places were also hidden. We will discuss this limitation in Section~\ref{limitation:preprocessing}.

  \textbf{Lemmatization:}\quad We used the \verb|nltk|~\cite{nltk} libaray to reduce words down to their lemma in the hopes of reducing the complexity within our text which may benefit feature extraction. This looks up the work in the WordNet corpus to get the lemma. Later in the research, we realised that this hypothesis may have not been accurate.

  Firstly the \verb|nltk| library we were using does not automatically detect the part of speech and will by default, only lemmatize nouns. While it is arguably better for us to maintain the tense of nouns, we are technically not lemmatizing fully. Secondly, from more research, lemmatization may not be ideal for BERT embeddings since it removes some semantics that could be learnt by the BERT model. We will discuss these limitations further in Section~\ref{limitation:preprocessing}.

  \textbf{Remove stopwords:}\quad Stopwords were removed from the text in order to reduce complexity.
\end{quote}

Apart from the above methods, we also tested removing punctuation. However, this was not used in the end since we added non-latent features to measure punctuation counts and also to maintain semantics for BERT.

After preprocessing, tokens are then generated based on any whitespace and punctuation in the remaining text. Table~\ref{preprocessing} shows samples of tokenized input articles.

\begin{table}
  \begin{center}
    \makebox[0pt]{\begin{minipage}{\paperwidth}
        \centering
        \subfloat{
          \begin{tabular}{cp{8cm}p{6cm}}
            \toprule
            ID & Article extract & Tokens\\
            \midrule
            118\_Real & \small{\articlecontent{118real}}
            & \small{['fbi', 'director', 'james', 'comey', 'said', 'sunday', 'bureau', 'change', 'conclusion', 'made', 'july', 'examined', 'newly', 'revealed', 'email', 'related', 'hillary', 'clinton', 'probe', '.', '"', 'based', 'review', ',', 'changed', 'conclusion', 'expressed', 'july', 'respect', 'secretary', 'clinton',\ldots]}\\
            \midrule
            15\_Fake & \small{\articlecontent{15fake}}
            & \small{['hearing', '200', 'marines', 'left', 'stranded', 'returning', 'home', 'operation', 'desert', 'storm', 'back', '1991', ',', 'donald', 'j', '.', 'trump', 'came', 'aid', 'marines', 'sending', 'one', 'plane', 'camp', 'lejuene', ',', 'north', 'carolina', 'transport', 'back', 'home', 'family', 'miami',\ldots]}\\
            \bottomrule
          \end{tabular}
        }
        \hfill
      \end{minipage}}
  \end{center}
  \caption{Examples of preprocessing and tokenization extraction on items in dataset.}
  \label{preprocessing}
\end{table}

\newsubsection{Feature --- Similarity model}

One of the core aspects of our research was the ability to automatically gather articles that give some context to each input article. Our approach summarizes the input article so it can be used to find contextual articles. These articles can then be used for comparison to the input article.

\subsubsection*{Summary extraction}

To get the context articles, we need to summarize the main topic of our input article down to at most 10 keywords. We use the Python \verb|gensim|~\cite{py-gensim} library which provides various topic modelling interfaces for text inputs. We use the \verb|ldamodel| which implements Latent Dirichlet Allocation (LDA) to extract a single topic. LDA is a probabilistic model where the idea is you have a number of documents representing some latent topics characterized by a distribution over words. By feeding in the preprocessed sentences of our input article, we are able to get the main themes. We sort the output keywords by the probability they represent the topic then cap the amount of words to 10 at most.

For the scope of our research, we are able to perform manual validation of the summaries extracted to check the summary represented the article content well. Table~\ref{summary-extraction} shows some samples of items in our dataset after applying LDA. We see that while the summaries extracted are not perfect, they still represent the general meaning of the article. Two common issues we saw were:
\begin{itemize}
  \item Unordered words in the summary --- words representing the topics seemed to be unordered. To a human reading the summary by itself, they might be able to see that the words are all keywords of the article but put together in a sentence, will not completely make sense. We hypothesize that this could have caused sub-optimal results when we started scraping articles using the summaries.
  \item Appearance of stop words and other meaningless non-topic words in the summary --- As a flow on issue from our preprocessing, our summary was left with words such as ``wa'' (from ``was'') or ``ha'' (from ``has''). This would have impacted the meaning of our summary and later article scraping.\label{summary-extraction:bad-words}
\end{itemize}
We will discuss the possibility of extracting better summaries using a more robust model in Section~\ref{limitation:summary-extraction}.

\begin{table}
  \centering
  \begin{tabular}{cp{8cm}p{3cm}}
    \toprule
    ID & Article extract & Summary\\
    \midrule
    118\_Real & \small{\articlecontent{118real}}
    & email review fbi clinton said july comey news new wa\\
    \midrule
    15\_Fake & \small{\articlecontent{15fake}}
    & home marines trump wa stickney way north plane family\\
    \bottomrule
  \end{tabular}
  \caption{Examples of summary extraction on items in dataset.}
  \label{summary-extraction}
\end{table}

\subsubsection*{Article scraping}

We feed the summary of the input article into Google News and collect the top three articles. We use Google News since it essentially provides a free PageRank algorithm which we can leverage to get the most popular articles during the time period. We will treat the articles we find as Real articles for purposes of comparison, i.e.\ an input article that is very different to our contextual article is likely to be Fake.

For our research, we will only manually feed in all summaries for our dataset. Our motivation for this research was to develop a tool that a user could potentially use to figure out if the current news they are reading contains misinformation. We acknowledge there exists APIs that provide either a wrapper around Google News or implement their own news search algorithm that we could have looked into. However, given the size of the dataset and our scope, this was not necessary to demonstrate our system.

\begin{quote}
  \textbf{SETUP:} We use a virtual machine with a freshly installed latest version of Google Chrome. Searches are condicted in ``Incognito Mode'' tabs. We also use a VPN to the West coast of the US. These invariants serve the main purpose so that Google's does not give any personalized results based on a browser fingerprint or IP address. We chose the US as the VPN destination since our dataset articles were extracted from US news sources and we wanted to scrape for articles with a similar style of writing. If you were to use the tool in Australia, Google would usually return articles from local sources. We restrict our scope to specifically this dataset rather than train on a wide dataset from all sources.

  Another invariant we implement is to add a \verb|before:2020| to our summary. This forces Google News to only find articles before this year so that the news we get won't be from recent news. A common discussion topic from our dataset was Donald Trump's 2016 election campaign and we know that the news regarding Trump in 2023 is much different to that of 2016. This makes sense as we are not using a very recent dataset so clamping the date we find contextual articles assumes that if were looking for fake articles at the time of reading the imput article, we wouldn't have too much future articles available.

  \textbf{PROCESS:} We attempt to get the top three articles and save the URL for each input article. Not all summaries returned three articles so we perform scraping in three passes:
  \begin{enumerate}
    \item We enter the whole summary without any changes. This is the most ideal approach and most machine-replicable. This covered 70\% of our dataset.
    \item Still performing only generic actions, we remove any~\hyperref[summary-extraction:bad-words]{\color{blue}bad words} or non-important connectives then searched again. This should still be machine-replicable with further work. This covered the next 20\% of our dataset.
    \item For the last 10\% of our dataset, we had to manually look at the input article content and summary generated to figure out why we still received no results. Our hypothesis was that this was a combination of our non-tuned summary extraction and the fact that some \emph{outrageous} Fake articles simply didn't have any similar articles that could be found. We will discuss this limitation in Section~\ref{limitation:article-scraping}.
  \end{enumerate}
  From the above passes, we were not able to find context articles for four input articles described in a table in Appendix~\ref{appendix:article-scraping}. Furthermore, we were only able to find one or two articles for some inputs but we can still continue with our similarity model.
\end{quote}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\textwidth]{img/chrome-article-scraping.png}
  \caption{Sample of articles found in Google after searching an article summary.}
\end{figure}

\noindent
After gathering three URL links for each context article, we use the Python \verb|newspaper3k|~\cite{py-newspaper} library to download the article and automatically extract its title and content.

\subsubsection*{Similarity model}

\tododuke{Write}

\newsubsection{Feature --- Non-latent features}

\tododuke{Write}

\newsubsection{Feature --- BERT embeddings}

\tododuke{Write}

\newsubsection{Normalization and scaling}

\tododhruv{Write}

\newsubsection{Model --- Machine learning}

We used four state of the art machine learning models (commonly used in fake news detection) to perform our classification. We chose Logistic Regression (LR), Support Vector Machines (SVM), Decision Trees (DT), and XGBoost (XGB). Due to the small size of our dataset, we needed to tune the regularization hyperparameters to ensure our models didn't overfit. In particular, tree-based models such as DT and XGB should be able to fully segment our classes so we need to control the depth of the tree and splitting criteria. Models such as LR and SVM will need to control L2 regularization. In SVM, we will test the type of kernel used.

To find the best hyperparameters, we perform 5-Fold cross validation across 80\% of our dataset and average the validation score. We pick the parameters with the best validation score and test our model with the remaining 20\% of the dataset. The table in the Appendix~\ref{apppendix:machine-learning} shows the hyperparameters tested for each model and a reason for why the ranges were selected.

\newsubsection{Model --- Neural networks}

\tododhruv{Write}

\newsection{Experimental setup}

\newsubsection{Dataset}

For our research, we use the \verb|FakeNewsData| dataset collated by Horne and Adali in~\cite{horne2017} on research regarding fake news in the 2016 presidential elections. This dataset contains two subsets, \emph{``Buzzfeed Political News''}, and \emph{``Random Political News''}. We make use of the \emph{Buzzfeed} subset since this contains long form text articles that are binary categorized in with Fake and Real labels. The \emph{Random} subset contains an extra label, Satire, which is out of scope for our research.

The original dataset was collated by Craig Silverman (BuzzFeed News Editor) in an article~\cite{dataset-buzzfeed} analyzing fake news. The analysis concentrates on the Facebook engagement on real and fake news articles shared to the social media website. Various keywords related to events during the election were searched and articles with highest engagement were collected. A ground truth was assigned by manual analysis using a list of known fake and hyperpartisan news sites. A details description of their process can be found in their article.

Following BuzzFeed's analysis, Horne extracted the content and title from the articles and formed the dataset. In total, there were 53 real and 48 fake articles. Notable events during the election such as Donald Trump's campaign and various Hillary Clinton scandals and rumors were features in the articles.

After extending this dataset with our novel context article scraping and similarity methods, we used a 60/20/20 train/validation/test set. This was stratified and randomized to ensure the best results. An example of items in our dataset can be found in Table~\ref{article-samples}.

\begin{table}
  \begin{center}
    \makebox[0pt]{\begin{minipage}{\paperwidth}
        \centering
        \subfloat{
          \begin{tabular}{p{8cm}p{8cm}}
            \toprule
            \textbf{118\_Real} & \textbf{1\_Fake}\\
            \midrule
            \emph{\articletitle{118real}} & \emph{\articletitle{1fake}}\\
            \midrule
            \small{\articlecontent{118real}} & \small{\articlecontent{1fake}}\\
            \bottomrule
          \end{tabular}
        }
        \hfill
      \end{minipage}}
  \end{center}
  \caption{A sample of one fake and real article in our dataset. The article ID, title and content are shown in the rows. Both articles are regarding a scandal with Hillary Clinton using a private server to store emails. The fake article reports on an event that never happens whereas the real article reports the true event -- that Clinton was exonerated from criminality.}
  \label{article-samples}
\end{table}

\newsubsection{Evaluation metrics}

To evaluate our classification models, we will use accuracy and F1 score. These metrics are commonly used for binary classification problems as well as in the misinformation detection domain.

Accuracy is measured as the proportion of the total number of correctly classified samples over the total count of samples:
$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$$
Our dataset is quite balanced so this will be a good general first step measure.

On the other hand, the F1 score is generally used on imbalanced datasets by looking at both precision and recall:
$$F_1 = 2\,\frac{\text{precision}\times\text{recall}}{\text{precision}+\text{recall}}$$
Whilst our dataset is not imbalanced, we will still output this result for comparative purposes.

\newsection{Results and discussion}

\todojim{Machine learning}

\tododhruv{Neural nets}

\newsection{Conclusion}

\todoany{Summarise the study and discuss directions for future improvement}

\newsubsection{Limitations}

\todoany{Convert list of limitations to subsubsections with discussion.}

\subsubsection*{Preprocessing and tokenization}\label{limitation:preprocessing}

While our preprocessing was quite generic for NLP tasks, we believe there were a few oversights that caused unreliable results. Two issues were:
\begin{itemize}
  \item Converting everything to lowercase destroyed acronyms such as ``US''. This changed the meaning of some sentences.
  \item The \verb|nltk| lemmatizer required manually specifying the part of speech to work. This caused some verbs to be incorrectly lemmatized. We could have used a different library that extracted the POS automatically. Alternatively we could have investigated not lemmatizing at all to maintain proper structure for non-latent features and BERT embeddings.
\end{itemize}

This research would have really benefitted from less or no preprocessing at all. For all our features, there could be an argument where no preprocessing would have been better. For example summary extraction or BERT features could have learnt meaning from the unfiltered text. This could be investigated for future research to strengthen our features.

\subsubsection*{Summary extraction}\label{limitation:summary-extraction}

Our summary extraction extracted most of the correct meaning from the text. However two main issues remained causing it to produce in-perfect results:
\begin{itemize}
  \item Words would be unordered and if read together in a sentence, wouldn't make sense to a human.
  \item \emph{Junk} such as connectives would be left in the summary.
\end{itemize}

We believe future research could have looked into better tuned models that synthesized higher quality topic sentences. In our research, there were very few but we believe in particular a summary extractor trained on long-form news articles would have performed better. In addition, we could have also included the title since semantically, the title is supposed to tell the reader what the rest of the article is about.

\subsubsection*{Article scraping}\label{limitation:article-scraping}

We had intended our pipeline to be fully automatic, using an API to scrape for articles based on the keywords. For the scope of this project, we settled on manual scraping to show that using contextual articles would improve results. Unfortunately, after being passed to Google, some input summaries would return no or limited results and required human intervention to produce any articles. This was a particular problem.

For Real labelled articles, we believe an improvement on the summary extractor reducing the amount of \emph{junk} returned would have improved results. However, we hypothesize that if an outrageous Fake article as introduced, we may very well find no results about the event online. One example is the following article about Trump \emph{``snorting cocaine''}:
\begin{quote}
  \textbf{10Fake:}\quad\articlecontent{10fake}
\end{quote}
This event never happened and consequently, we could not find any contextual articles about it. For our research, we skipped articles with no context. We believe one way to resolve this is to include the article with no similarity score or a low one. More research needs to be done into whether mixed articles with and without a similarity can perform well together especially considering in the real world, this is definatly an issue.

\subsubsection*{Dataset}

For the scope of our research, we used a fairly small dataset to present our contributions to contextual article scraping. However, this dataset only concentrated on the political events surrounding the 2016 United States election. This causes two main problems for our model:
\begin{itemize}
  \item We are prone to overfit our models since such a small dataset will be easily segmented by most state of the art methods.
  \item Our model will learn specifics in the US election which we do not want to learn. This could cause the models to be confused if we try to classify more recent news.
\end{itemize}

In the future research could be done to provide an automated API for scraping which would have allowed for a much larger dataset to be used that covers multiple world events across different years. Along with this, masking out event specific words could have been done to reduce learning of event specifics.

\cleardoublepage
\pagebreak

\nocite{*}
\printbibliography

\cleardoublepage
\pagebreak

\appendix

\newsection{Individual contributions}

\begin{table}[H]
  \centering
  \begin{tabular}{lll}
    Jim & Dhruv & Duke \\
    \midrule
  \end{tabular}
\end{table}

\subsection{Jim}

\todojim{$\sim$1pg detailing individual contributions}

\subsection{Dhruv}

\tododhruv{$\sim$1pg detailing individual contributions}

\subsection{Duke}

\tododuke{$\sim$1pg detailing individual contributions}

\newsection{Article scraping}\label{appendix:article-scraping}

\begin{table}[H]
  \centering
  \begin{tabular}{cp{8cm}p{3cm}}
    \toprule
    ID & Article extract & Summary\\
    \midrule
    \verb|128_Real| & \small{\articlecontent{128real}} & god wa one never every king november still heart\\
    \midrule
    \verb|2_Fake| & \small{\articlecontent{2fake}} & ha adenugame africanamericans south femi united states africa president donald\\
    \midrule
    \verb|10_Fake| & \small{\articlecontent{10fake}} &wa room hotel maria told employee gonzalez hit video get\\
    \midrule
    \verb|34_Fake| & \small{\articlecontent{34fake}} &trump rage album machine band ha donald music outside year\\
    \bottomrule
  \end{tabular}
  \caption{Articles we were not able to find context articles for.}
\end{table}

\newsection{Machine learning}\label{apppendix:machine-learning}

\begin{table}[H]
  \centering
  \begin{tabular}{cL{2cm}L{2.5cm}p{7cm}}
    \toprule
    Model & Parameter & Selection & Reasoning\\
    \midrule
    \multirow{2}{*}{\shortstack{Logistic\\Regression}}& Inverse L2 coefficient & 0.2:1.2:0.2 & This is the main regularization parameter. We chose a range around the default 1.0 but shifted our range to be more biased towards higher regularization.\\
    \cmidrule{2-4}
    & Solver & lbfgs, liblinear & The liblinear solver was suggested by the documentation as an alternative for small datasets.\\
    \midrule
    \multirow{4}{*}{SVM}& Inverse L2 coefficient & 0.2:1.2:0.2 & Same reasoning as LR regularization.\\
    \cmidrule{2-4}
    & Kernel & rbf, poly, sigmoid & Selecting the right kernel for a dataset will make our methods perform better.\\
    \cmidrule{2-4}
    & Kernel coefficient & $\frac{1}{\text{n\_features}\times var(X)}$, 0.01, 0.05 & Same reason as above.\\
    \midrule
    \multirow{4}{*}{Decision Tree}& Criterion & gini, entropy & To test different methods of measuring split quality on the node.\\
    \cmidrule{2-4}
    & Max depth & no limit, 3:9:2 & Controls how complex the tree is. A less deeper tree is more regularized.\\
    \cmidrule{2-4}
    & Max features & $0.3\times \text{n\_features}$, $\sqrt{\text{n\_features}}$, all features & Standard defaults suggested by documentation. Is a regularization control so not all features are considered at each split.\\
    \cmidrule{2-4}
    & Min samples for splitting node & 2:4:1 & Reduce the number of leafs with only one sample of representation to increase regularization.\\
    \midrule
    \multirow{4}{*}{XGBoost}& Learning rate & 0.1:0.5:0.1 & Smaller learning rates reduce overfitting.\\
    \cmidrule{2-4}
    & Max depth & 1:6:1 & Same as max depth for DTs.\\
    \cmidrule{2-4}
    & L2 coefficient & 0.8:1.6:0.2 & Testing higher regularization. Default is 1.\\
    \cmidrule{2-4}
    & L1 coefficient & 0:0.4:0.2 & Testing higher regularization. Default is 0.0.\\
    \bottomrule
  \end{tabular}
  \caption{Table of all the models chosen and the hyperparameters selected for each model. We describe a range of values in the format start:end:step, where start and end are inclusive.}
\end{table}

\end{document}
